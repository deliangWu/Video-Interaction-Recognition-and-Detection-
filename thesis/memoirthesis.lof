\select@language {english}
\par \penalty \@M \textbf {{\scshape Figure} \hfill Page}\par \penalty \@M 
\addvspace {10pt}
\contentsline {figure}{\numberline {1.1}{\ignorespaces Illustration of some challenges of video analysis. }}{3}{figure.1.1}
\addvspace {10pt}
\contentsline {figure}{\numberline {2.1}{\ignorespaces The hierarchical activity recognition model and an example. Reprinted from \cite {choi2012}.}}{6}{figure.2.1}
\contentsline {figure}{\numberline {2.2}{\ignorespaces The SIFT descriptor. The left image shows the 2D SIFT descriptor. The center image shows how multiple 2D SIFT descriptor could be used on a video without modification to the original method. The right image shows the 3D SIFT descriptor with its 3D sub-volumes, each sub-volume is accumulated into its own sub-histogram. These histograms are what makes up the final descriptor. Reprinted from \cite {grepory2010}.}}{8}{figure.2.2}
\contentsline {figure}{\numberline {2.3}{\ignorespaces Overview of 3D-HOG descriptor computation; (a) the support region around a point of interest is divided into a grid of gradient orientation histograms; (b) each histogram is computed over a grid of mean gradients; (c) each gradient orientation is quantized using regular polyhedrons; (d) each mean gradient is computed using integral videos. Reprinted from \cite {alex2008}.}}{9}{figure.2.3}
\contentsline {figure}{\numberline {2.4}{\ignorespaces Illustration of DT algorithm to extract and characterize dense trajectories. Left: Feature points are densely sampled on a grid for each spatial scale. Middle: Tracking is carried out in the corresponding spatial scale for L frames by median filtering in a dense optical flow field. Right: The trajectory shape is represented by relative point coordinates, and the descriptors (HOG, HOF, MBH) are computed along the trajectory in a \(N * N\) pixels neighbourhood, which is divided into \(n\sigma * n\sigma * n\tau \) cells. Reprinted from \cite {wang2012}.}}{10}{figure.2.4}
\contentsline {figure}{\numberline {2.5}{\ignorespaces The architecture of a CNN example: LeNet5. Reprinted from \cite {cnn}.}}{12}{figure.2.5}
\contentsline {figure}{\numberline {2.6}{\ignorespaces The intuitive illustration of convolution over image.}}{12}{figure.2.6}
\contentsline {figure}{\numberline {2.7}{\ignorespaces Explored approaches for fusing information over temporal dimension through the network. Red, green and blue boxes indicate convolutional, normalization and pooling layers respectively. In the Slow Fusion model, the depicted columns share parameters. Reprinted from \cite {karpathy2014}.}}{14}{figure.2.7}
\contentsline {figure}{\numberline {2.8}{\ignorespaces The architecture of Two Stream ConvNet. Reprinted from \cite {simonyan2014}.}}{14}{figure.2.8}
\contentsline {figure}{\numberline {2.9}{\ignorespaces 2D and 3D convolution operations. a) Applying 2D convolution on an image results in an image. b) Applying 2D convolution on a video volume (multiple frames as multiple channels) also results in an image. c) Applying 3D convolution on a video volume results in another volume, preserving temporal information of the input signal. Reprinted from \cite {Tran2015}.}}{15}{figure.2.9}
\contentsline {figure}{\numberline {2.10}{\ignorespaces The architecture of C3D. C3D has 8 convolution, 5 max-pooling, and 2 full connected layers, followed by a softmax output layer. All 3D convolution kernels are \(3 \times 3 \times 3\) with stride 1 in both spatial and temporal dimensions. Number of filters are denoted in each box. The 3D pooling layers are denoted from pool1 to pool5. All pooling kernels are \(2 \times 2 \times 2\) except for pool1 is \(1 \times 2 \times 2\). Each fully connected layer has 4096 output units. Reprinted from \cite {Tran2015}.}}{15}{figure.2.10}
\contentsline {figure}{\numberline {2.11}{\ignorespaces Visualization of C3D model. Interestingly, C3D captures appearance for the first few frames but thereafter only attends to salient motion. Reprinted from \cite {Tran2015}.}}{15}{figure.2.11}
\contentsline {figure}{\numberline {2.12}{\ignorespaces The illustration of the 6 classes of human-human interactions in the UT-Interaction dataset.}}{19}{figure.2.12}
\contentsline {figure}{\numberline {2.13}{\ignorespaces The illustration of the different backgrounds between the two sets}}{20}{figure.2.13}
\contentsline {figure}{\numberline {2.14}{\ignorespaces Some frames randomly sampled from the 120 segmented videos. }}{20}{figure.2.14}
\contentsline {figure}{\numberline {2.15}{\ignorespaces Illustration of detection and ground truth. }}{21}{figure.2.15}
\contentsline {figure}{\numberline {2.16}{\ignorespaces Some challenges for the classification task. }}{22}{figure.2.16}
\contentsline {figure}{\numberline {2.17}{\ignorespaces Illustrations of some extra challenges for the detection task. }}{22}{figure.2.17}
\addvspace {10pt}
\contentsline {figure}{\numberline {3.1}{\ignorespaces Overall framework of the interaction classification. }}{24}{figure.3.1}
\contentsline {figure}{\numberline {3.2}{\ignorespaces Overall framework of the interaction detection. }}{25}{figure.3.2}
\contentsline {figure}{\numberline {3.3}{\ignorespaces The architecture of the person segmentation}}{26}{figure.3.3}
\contentsline {figure}{\numberline {3.4}{\ignorespaces The architecture of the person segmentation}}{27}{figure.3.4}
\contentsline {figure}{\numberline {3.5}{\ignorespaces Overall Architecture of the 3D-ConvNet. }}{27}{figure.3.5}
\addvspace {10pt}
\contentsline {figure}{\numberline {4.1}{\ignorespaces The overall data-flow of the interaction classification}}{32}{figure.4.1}
\contentsline {figure}{\numberline {4.2}{\ignorespaces The overall data-flow of the data pre-processing module}}{33}{figure.4.2}
\contentsline {figure}{\numberline {4.3}{\ignorespaces Horizontal flipping of a video}}{38}{figure.4.3}
\contentsline {figure}{\numberline {4.4}{\ignorespaces Random cropping a video}}{38}{figure.4.4}
\contentsline {figure}{\numberline {4.5}{\ignorespaces The structure of the feature descriptor}}{39}{figure.4.5}
\contentsline {figure}{\numberline {4.6}{\ignorespaces The architecture of the 3D ConvNet}}{39}{figure.4.6}
\contentsline {figure}{\numberline {4.7}{\ignorespaces The illustration of the ReLU operation}}{40}{figure.4.7}
\contentsline {figure}{\numberline {4.8}{\ignorespaces The softmax classifier}}{43}{figure.4.8}
\contentsline {figure}{\numberline {4.9}{\ignorespaces The data-flow of interaction detection}}{45}{figure.4.9}
\contentsline {figure}{\numberline {4.10}{\ignorespaces Illustrations of notations of bounding boxes, a). notations of a bounding boxes of an individual and b). notations of a bounding box of two-person interaction.}}{46}{figure.4.10}
\contentsline {figure}{\numberline {4.11}{\ignorespaces Some examples of the horizon positions of the interacting people, sampled from the unsegmented videos of UT-Interaction dataset.}}{47}{figure.4.11}
\contentsline {figure}{\numberline {4.12}{\ignorespaces An example illustration of preserving and discarding bounding boxes according to the value of \(Y_{Imean}\)}}{49}{figure.4.12}
\contentsline {figure}{\numberline {4.13}{\ignorespaces An example illustration of preserving and discarding bounding boxes according to the value of \(X_{Imean}\)}}{51}{figure.4.13}
\contentsline {figure}{\numberline {4.14}{\ignorespaces The illustration of temporally sliding the 3D windows which are used to generate candidates of interaction video clips.}}{52}{figure.4.14}
\contentsline {figure}{\numberline {4.15}{\ignorespaces A diagram of temporally combining of interaction video clips }}{53}{figure.4.15}
\addvspace {10pt}
\addvspace {10pt}
