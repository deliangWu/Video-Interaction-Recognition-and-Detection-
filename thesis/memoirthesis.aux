\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand*{\memsetcounter}[2]{}
\catcode `"\active 
\catcode `<\active 
\catcode `>\active 
\@nameuse{es@quoting}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\par \penalty \@M \unhbox \voidb@x \hbox {}\hfill {\nag@@warning@vi  \bfseries  Page}\par \penalty \@M }
\@writefile{toc}{\contentsline {chapter}{List of Tables}{vii}{section*.3}}
\@writefile{lot}{\par \penalty \@M \textbf  {{\scshape  Table} \hfill Page}\par \penalty \@M }
\citation{choi2012}
\citation{grepory2010}
\citation{alex2008}
\citation{wang2012}
\citation{cnn}
\citation{karpathy2014}
\citation{simonyan2014}
\citation{Tran2015}
\@writefile{toc}{\contentsline {chapter}{List of Figures}{ix}{section*.4}}
\citation{Tran2015}
\citation{Tran2015}
\@writefile{lof}{\par \penalty \@M \textbf  {{\scshape  Figure} \hfill Page}\par \penalty \@M }
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {1}Introduction}{1}{chapter.1}}
\newlabel{chap:intro}{{\M@TitleReference {1}{Introduction}}{1}{Introduction}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Background}{1}{section.1.1}}
\newlabel{sec:intro_sec01}{{\M@TitleReference {1.1}{Background}}{1}{Background}{section.1.1}{}}
\citation{patron2010}
\citation{Gemeren2015}
\citation{narayan2014}
\citation{choi2012}
\citation{patron2010}
\citation{Gemeren2015}
\citation{hog}
\citation{hof}
\citation{narayan2014}
\citation{choi2012}
\citation{bov}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Illustration of some challenges of video analysis. }}{3}{figure.1.1}}
\newlabel{fig:challenges}{{\M@TitleReference {1.1}{Illustration of some challenges of video analysis. }}{3}{Illustration of some challenges of video analysis. }{figure.1.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Project Goals}{3}{section.1.2}}
\newlabel{sec:intro_sec02}{{\M@TitleReference {1.2}{Project Goals}}{3}{Project Goals}{section.1.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Contributions}{3}{section.1.3}}
\newlabel{sec:intro_sec03}{{\M@TitleReference {1.3}{Contributions}}{3}{Contributions}{section.1.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Outline}{4}{section.1.4}}
\newlabel{sec:intro_outline}{{\M@TitleReference {1.4}{Outline}}{4}{Outline}{section.1.4}{}}
\citation{patron2010}
\citation{Gemeren2015}
\citation{narayan2014}
\citation{choi2012}
\citation{Ji2013}
\citation{Ng2015}
\citation{Tran2015}
\citation{alex2008}
\citation{grepory2010}
\citation{karpathy2014}
\citation{simonyan2014}
\citation{choi2012}
\citation{choi2012}
\citation{bov}
\citation{choi2012}
\citation{choi2012}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {2}Related Work}{5}{chapter.2}}
\newlabel{chap2}{{\M@TitleReference {2}{Related Work}}{5}{Related Work}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Architectures of Interaction video analysis related works}{5}{section.2.1}}
\newlabel{2_1}{{\M@TitleReference {2.1}{Architectures of Interaction video analysis related works}}{5}{Architectures of Interaction video analysis related works}{section.2.1}{}}
\citation{Gemeren2015}
\citation{patron2010}
\citation{Gemeren2015}
\citation{yimeng}
\citation{yu}
\citation{lowe1999}
\citation{lowe2004}
\citation{hog}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces The hierarchical activity recognition model and an example. Reprinted from \cite  {choi2012}.}}{6}{figure.2.1}}
\newlabel{fig:hierA}{{\M@TitleReference {2.1}{The hierarchical activity recognition model and an example. Reprinted from \cite  {choi2012}.}}{6}{The hierarchical activity recognition model and an example. Reprinted from \cite {choi2012}}{figure.2.1}{}}
\citation{grepory2010}
\citation{paul2007}
\citation{alex2008}
\citation{wang2012}
\citation{wang2013}
\citation{lowe1999}
\citation{lowe2004}
\citation{grepory2010}
\citation{paul2007}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Hand-crafted Feature Descriptor}{7}{section.2.2}}
\newlabel{2_2}{{\M@TitleReference {2.2}{Hand-crafted Feature Descriptor}}{7}{Hand-crafted Feature Descriptor}{section.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}3D-SIFT Feature Descriptor}{7}{subsection.2.2.1}}
\newlabel{2_2_1}{{\M@TitleReference {2.2.1}{3D-SIFT Feature Descriptor}}{7}{3D-SIFT Feature Descriptor}{subsection.2.2.1}{}}
\citation{grepory2010}
\citation{grepory2010}
\citation{alex2008}
\citation{alex2008}
\citation{alex2008}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces The SIFT descriptor. The left image shows the 2D SIFT descriptor. The center image shows how multiple 2D SIFT descriptor could be used on a video without modification to the original method. The right image shows the 3D SIFT descriptor with its 3D sub-volumes, each sub-volume is accumulated into its own sub-histogram. These histograms are what makes up the final descriptor. Reprinted from \cite  {grepory2010}.}}{8}{figure.2.2}}
\newlabel{fig:3DSIFT}{{\M@TitleReference {2.2}{The SIFT descriptor. The left image shows the 2D SIFT descriptor. The center image shows how multiple 2D SIFT descriptor could be used on a video without modification to the original method. The right image shows the 3D SIFT descriptor with its 3D sub-volumes, each sub-volume is accumulated into its own sub-histogram. These histograms are what makes up the final descriptor. Reprinted from \cite  {grepory2010}.}}{8}{The SIFT descriptor. The left image shows the 2D SIFT descriptor. The center image shows how multiple 2D SIFT descriptor could be used on a video without modification to the original method. The right image shows the 3D SIFT descriptor with its 3D sub-volumes, each sub-volume is accumulated into its own sub-histogram. These histograms are what makes up the final descriptor. Reprinted from \cite {grepory2010}}{figure.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}3D-HOG Feature descriptor}{8}{subsection.2.2.2}}
\newlabel{2_2_2}{{\M@TitleReference {2.2.2}{3D-HOG Feature descriptor}}{8}{3D-HOG Feature descriptor}{subsection.2.2.2}{}}
\citation{wang2012}
\citation{wang2013}
\citation{wang2012}
\citation{wang2013}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces  Overview of 3D-HOG descriptor computation; (a) the support region around a point of interest is divided into a grid of gradient orientation histograms; (b) each histogram is computed over a grid of mean gradients; (c) each gradient orientation is quantized using regular polyhedrons; (d) each mean gradient is computed using integral videos. Reprinted from \cite  {alex2008}.}}{9}{figure.2.3}}
\newlabel{fig:3DHOG}{{\M@TitleReference {2.3}{ Overview of 3D-HOG descriptor computation; (a) the support region around a point of interest is divided into a grid of gradient orientation histograms; (b) each histogram is computed over a grid of mean gradients; (c) each gradient orientation is quantized using regular polyhedrons; (d) each mean gradient is computed using integral videos. Reprinted from \cite  {alex2008}.}}{9}{ Overview of 3D-HOG descriptor computation; (a) the support region around a point of interest is divided into a grid of gradient orientation histograms; (b) each histogram is computed over a grid of mean gradients; (c) each gradient orientation is quantized using regular polyhedrons; (d) each mean gradient is computed using integral videos. Reprinted from \cite {alex2008}}{figure.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Improved Dense Trajectories feature descriptor}{9}{subsection.2.2.3}}
\newlabel{2_2_3}{{\M@TitleReference {2.2.3}{Improved Dense Trajectories feature descriptor}}{9}{Improved Dense Trajectories feature descriptor}{subsection.2.2.3}{}}
\citation{hog}
\citation{hof}
\citation{hof}
\citation{wang2012}
\citation{wang2012}
\citation{wang2013}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Illustration of DT algorithm to extract and characterize dense trajectories. Left: Feature points are densely sampled on a grid for each spatial scale. Middle: Tracking is carried out in the corresponding spatial scale for L frames by median filtering in a dense optical flow field. Right: The trajectory shape is represented by relative point coordinates, and the descriptors (HOG, HOF, MBH) are computed along the trajectory in a \(N * N\) pixels neighbourhood, which is divided into \(n\sigma * n\sigma * n\tau \) cells. Reprinted from \cite  {wang2012}.}}{10}{figure.2.4}}
\newlabel{fig:DT}{{\M@TitleReference {2.4}{Illustration of DT algorithm to extract and characterize dense trajectories. Left: Feature points are densely sampled on a grid for each spatial scale. Middle: Tracking is carried out in the corresponding spatial scale for L frames by median filtering in a dense optical flow field. Right: The trajectory shape is represented by relative point coordinates, and the descriptors (HOG, HOF, MBH) are computed along the trajectory in a \(N * N\) pixels neighbourhood, which is divided into \(n\sigma * n\sigma * n\tau \) cells. Reprinted from \cite  {wang2012}.}}{10}{Illustration of DT algorithm to extract and characterize dense trajectories. Left: Feature points are densely sampled on a grid for each spatial scale. Middle: Tracking is carried out in the corresponding spatial scale for L frames by median filtering in a dense optical flow field. Right: The trajectory shape is represented by relative point coordinates, and the descriptors (HOG, HOF, MBH) are computed along the trajectory in a \(N * N\) pixels neighbourhood, which is divided into \(n\sigma * n\sigma * n\tau \) cells. Reprinted from \cite {wang2012}}{figure.2.4}{}}
\citation{cnn}
\citation{kaiming}
\citation{cnn}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Deep Learning Based Feature Descriptor}{11}{section.2.3}}
\newlabel{2_3}{{\M@TitleReference {2.3}{Deep Learning Based Feature Descriptor}}{11}{Deep Learning Based Feature Descriptor}{section.2.3}{}}
\citation{cnn}
\citation{cnn}
\citation{ning2005}
\citation{karpathy2014}
\citation{simonyan2014}
\citation{3dcnn_1}
\citation{Ji2013}
\citation{Tran2015}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces The architecture of a CNN example: LeNet5. Reprinted from \cite  {cnn}.}}{12}{figure.2.5}}
\newlabel{fig:cnn}{{\M@TitleReference {2.5}{The architecture of a CNN example: LeNet5. Reprinted from \cite  {cnn}.}}{12}{The architecture of a CNN example: LeNet5. Reprinted from \cite {cnn}}{figure.2.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces The intuitive illustration of convolution over image.}}{12}{figure.2.6}}
\newlabel{fig:conv}{{\M@TitleReference {2.6}{The intuitive illustration of convolution over image.}}{12}{The intuitive illustration of convolution over image}{figure.2.6}{}}
\citation{karpathy2014}
\citation{karpathy2014}
\citation{kriz2012}
\citation{karpathy2014}
\citation{karpathy2014}
\citation{karpathy2014}
\citation{simonyan2014}
\citation{karpathy2014}
\citation{simonyan2014}
\citation{simonyan2014}
\citation{Ji2013}
\citation{Tran2015}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Spatial-Temporal CNNs feature descriptor}{13}{subsection.2.3.1}}
\newlabel{2_3_1}{{\M@TitleReference {2.3.1}{Spatial-Temporal CNNs feature descriptor}}{13}{Spatial-Temporal CNNs feature descriptor}{subsection.2.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Two-Stream ConvNet feature descriptor}{13}{subsection.2.3.2}}
\newlabel{2_3_2}{{\M@TitleReference {2.3.2}{Two-Stream ConvNet feature descriptor}}{13}{Two-Stream ConvNet feature descriptor}{subsection.2.3.2}{}}
\citation{Tran2015}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Explored approaches for fusing information over temporal dimension through the network. Red, green and blue boxes indicate convolutional, normalization and pooling layers respectively. In the Slow Fusion model, the depicted columns share parameters. Reprinted from \cite  {karpathy2014}.}}{14}{figure.2.7}}
\newlabel{fig:STCNNs}{{\M@TitleReference {2.7}{Explored approaches for fusing information over temporal dimension through the network. Red, green and blue boxes indicate convolutional, normalization and pooling layers respectively. In the Slow Fusion model, the depicted columns share parameters. Reprinted from \cite  {karpathy2014}.}}{14}{Explored approaches for fusing information over temporal dimension through the network. Red, green and blue boxes indicate convolutional, normalization and pooling layers respectively. In the Slow Fusion model, the depicted columns share parameters. Reprinted from \cite {karpathy2014}}{figure.2.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces The architecture of Two Stream ConvNet. Reprinted from \cite  {simonyan2014}.}}{14}{figure.2.8}}
\newlabel{fig:tsconvnet_1}{{\M@TitleReference {2.8}{The architecture of Two Stream ConvNet. Reprinted from \cite  {simonyan2014}.}}{14}{The architecture of Two Stream ConvNet. Reprinted from \cite {simonyan2014}}{figure.2.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}3D ConvNet feature descriptor}{14}{subsection.2.3.3}}
\newlabel{2_3_3}{{\M@TitleReference {2.3.3}{3D ConvNet feature descriptor}}{14}{3D ConvNet feature descriptor}{subsection.2.3.3}{}}
\citation{zeiler2014}
\citation{Tran2015}
\citation{Tran2015}
\citation{Tran2015}
\citation{Tran2015}
\citation{Tran2015}
\citation{Tran2015}
\citation{kth}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces 2D and 3D convolution operations. a) Applying 2D convolution on an image results in an image. b) Applying 2D convolution on a video volume (multiple frames as multiple channels) also results in an image. c) Applying 3D convolution on a video volume results in another volume, preserving temporal information of the input signal. Reprinted from \cite  {Tran2015}.}}{15}{figure.2.9}}
\newlabel{fig:3DConv}{{\M@TitleReference {2.9}{2D and 3D convolution operations. a) Applying 2D convolution on an image results in an image. b) Applying 2D convolution on a video volume (multiple frames as multiple channels) also results in an image. c) Applying 3D convolution on a video volume results in another volume, preserving temporal information of the input signal. Reprinted from \cite  {Tran2015}.}}{15}{2D and 3D convolution operations. a) Applying 2D convolution on an image results in an image. b) Applying 2D convolution on a video volume (multiple frames as multiple channels) also results in an image. c) Applying 3D convolution on a video volume results in another volume, preserving temporal information of the input signal. Reprinted from \cite {Tran2015}}{figure.2.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces The architecture of C3D. C3D has 8 convolution, 5 max-pooling, and 2 full connected layers, followed by a softmax output layer. All 3D convolution kernels are \(3 \times 3 \times 3\) with stride 1 in both spatial and temporal dimensions. Number of filters are denoted in each box. The 3D pooling layers are denoted from pool1 to pool5. All pooling kernels are \(2 \times 2 \times 2\) except for pool1 is \(1 \times 2 \times 2\). Each fully connected layer has 4096 output units. Reprinted from \cite  {Tran2015}.}}{15}{figure.2.10}}
\newlabel{fig:3DConvNet}{{\M@TitleReference {2.10}{The architecture of C3D. C3D has 8 convolution, 5 max-pooling, and 2 full connected layers, followed by a softmax output layer. All 3D convolution kernels are \(3 \times 3 \times 3\) with stride 1 in both spatial and temporal dimensions. Number of filters are denoted in each box. The 3D pooling layers are denoted from pool1 to pool5. All pooling kernels are \(2 \times 2 \times 2\) except for pool1 is \(1 \times 2 \times 2\). Each fully connected layer has 4096 output units. Reprinted from \cite  {Tran2015}.}}{15}{The architecture of C3D. C3D has 8 convolution, 5 max-pooling, and 2 full connected layers, followed by a softmax output layer. All 3D convolution kernels are \(3 \times 3 \times 3\) with stride 1 in both spatial and temporal dimensions. Number of filters are denoted in each box. The 3D pooling layers are denoted from pool1 to pool5. All pooling kernels are \(2 \times 2 \times 2\) except for pool1 is \(1 \times 2 \times 2\). Each fully connected layer has 4096 output units. Reprinted from \cite {Tran2015}}{figure.2.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces Visualization of C3D model. Interestingly, C3D captures appearance for the first few frames but thereafter only attends to salient motion. Reprinted from \cite  {Tran2015}.}}{15}{figure.2.11}}
\newlabel{fig:3DConvNetV}{{\M@TitleReference {2.11}{Visualization of C3D model. Interestingly, C3D captures appearance for the first few frames but thereafter only attends to salient motion. Reprinted from \cite  {Tran2015}.}}{15}{Visualization of C3D model. Interestingly, C3D captures appearance for the first few frames but thereafter only attends to salient motion. Reprinted from \cite {Tran2015}}{figure.2.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Datasets}{15}{section.2.4}}
\newlabel{2_4}{{\M@TitleReference {2.4}{Datasets}}{15}{Datasets}{section.2.4}{}}
\citation{ucf101}
\citation{aslan}
\citation{activitynet200}
\citation{kth}
\citation{marszalek09}
\citation{ucf101}
\citation{activitynet200}
\citation{ut2010}
\citation{shakefive2}
\citation{m2i_tju}
\citation{ut2010}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}List of human activity video datasets}{16}{subsection.2.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}UT-Interaction dataset}{16}{subsection.2.4.2}}
\newlabel{ut-interaction}{{\M@TitleReference {2.4.2}{UT-Interaction dataset}}{16}{UT-Interaction dataset}{subsection.2.4.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces List of human action video datasets}}{17}{table.2.1}}
\newlabel{table:action_datasets}{{\M@TitleReference {2.1}{List of human action video datasets}}{17}{List of human action video datasets}{table.2.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces List of human interaction video datasets}}{18}{table.2.2}}
\newlabel{table:interaction_datasets}{{\M@TitleReference {2.2}{List of human interaction video datasets}}{18}{List of human interaction video datasets}{table.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces The illustration of the 6 classes of human-human interactions in the UT-Interaction dataset.}}{19}{figure.2.12}}
\newlabel{fig:ut_6classes}{{\M@TitleReference {2.12}{The illustration of the 6 classes of human-human interactions in the UT-Interaction dataset.}}{19}{The illustration of the 6 classes of human-human interactions in the UT-Interaction dataset}{figure.2.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.13}{\ignorespaces The illustration of the different backgrounds between the two sets}}{20}{figure.2.13}}
\newlabel{fig:ut_2sets}{{\M@TitleReference {2.13}{The illustration of the different backgrounds between the two sets}}{20}{The illustration of the different backgrounds between the two sets}{figure.2.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.14}{\ignorespaces Some frames randomly sampled from the 120 segmented videos. }}{20}{figure.2.14}}
\newlabel{fig:ut_segments}{{\M@TitleReference {2.14}{Some frames randomly sampled from the 120 segmented videos. }}{20}{Some frames randomly sampled from the 120 segmented videos. }{figure.2.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.15}{\ignorespaces Illustration of detection and ground truth. }}{21}{figure.2.15}}
\newlabel{fig:ut_det}{{\M@TitleReference {2.15}{Illustration of detection and ground truth. }}{21}{Illustration of detection and ground truth. }{figure.2.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.16}{\ignorespaces Some challenges for the classification task. }}{22}{figure.2.16}}
\newlabel{fig:ut_challenges_1}{{\M@TitleReference {2.16}{Some challenges for the classification task. }}{22}{Some challenges for the classification task. }{figure.2.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.17}{\ignorespaces Illustrations of some extra challenges for the detection task. }}{22}{figure.2.17}}
\newlabel{fig:ut_challenges_2}{{\M@TitleReference {2.17}{Illustrations of some extra challenges for the detection task. }}{22}{Illustrations of some extra challenges for the detection task. }{figure.2.17}{}}
\citation{patron2010}
\citation{narayan2014}
\citation{choi2012}
\citation{narayan2014}
\citation{patron2010}
\citation{choi2012}
\citation{patron2010}
\citation{Ji2013}
\citation{Tran2015}
\citation{simonyan2014}
\citation{Ng2015}
\citation{grepory2010}
\citation{alex2008}
\citation{paul2007}
\citation{wang2012}
\citation{wang2013}
\citation{choi2012}
\citation{patron2010}
\citation{choi2012}
\citation{patron2010}
\citation{hog}
\citation{bov}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {3}Architecture}{23}{chapter.3}}
\newlabel{chap3}{{\M@TitleReference {3}{Architecture}}{23}{Architecture}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Overall Framework}{23}{section.3.1}}
\newlabel{3_1}{{\M@TitleReference {3.1}{Overall Framework}}{23}{Overall Framework}{section.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Interaction classification}{23}{subsection.3.1.1}}
\newlabel{arch_classification}{{\M@TitleReference {3.1.1}{Interaction classification}}{23}{Interaction classification}{subsection.3.1.1}{}}
\citation{ucf101}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Overall framework of the interaction classification. }}{24}{figure.3.1}}
\newlabel{fig:arch_classification}{{\M@TitleReference {3.1}{Overall framework of the interaction classification. }}{24}{Overall framework of the interaction classification. }{figure.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Interaction detection}{24}{subsection.3.1.2}}
\citation{inria_person}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Overall framework of the interaction detection. }}{25}{figure.3.2}}
\newlabel{fig:arch_det}{{\M@TitleReference {3.2}{Overall framework of the interaction detection. }}{25}{Overall framework of the interaction detection. }{figure.3.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Models}{25}{section.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Person Segmentation}{25}{subsection.3.2.1}}
\newlabel{person_segmentation}{{\M@TitleReference {3.2.1}{Person Segmentation}}{25}{Person Segmentation}{subsection.3.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces The architecture of the person segmentation}}{26}{figure.3.3}}
\newlabel{fig:person_detection}{{\M@TitleReference {3.3}{The architecture of the person segmentation}}{26}{The architecture of the person segmentation}{figure.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Spatial detection of interacting people}{26}{subsection.3.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Feature Descriptor}{26}{subsection.3.2.3}}
\newlabel{3_3}{{\M@TitleReference {3.2.3}{Feature Descriptor}}{26}{Feature Descriptor}{subsection.3.2.3}{}}
\citation{Tran2015}
\citation{Tran2015}
\citation{inria_person}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces The architecture of the person segmentation}}{27}{figure.3.4}}
\newlabel{fig:ip_det}{{\M@TitleReference {3.4}{The architecture of the person segmentation}}{27}{The architecture of the person segmentation}{figure.3.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Overall Architecture of the 3D-ConvNet. }}{27}{figure.3.5}}
\newlabel{fig:c3d}{{\M@TitleReference {3.5}{Overall Architecture of the 3D-ConvNet. }}{27}{Overall Architecture of the 3D-ConvNet. }{figure.3.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Training}{27}{section.3.3}}
\newlabel{3_4}{{\M@TitleReference {3.3}{Training}}{27}{Training}{section.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Train the person detection network}{27}{subsection.3.3.1}}
\citation{ucf101}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Train the feature descriptor}{28}{subsection.3.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Train the Classifier}{29}{subsection.3.3.3}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {4}Design}{31}{chapter.4}}
\newlabel{chap4}{{\M@TitleReference {4}{Design}}{31}{Design}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Interaction Classification}{31}{section.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Data-flow of the Interaction Classification}{31}{subsection.4.1.1}}
\citation{hog}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces The overall data-flow of the interaction classification}}{32}{figure.4.1}}
\newlabel{fig:df_classifier}{{\M@TitleReference {4.1}{The overall data-flow of the interaction classification}}{32}{The overall data-flow of the interaction classification}{figure.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Data pre-processing}{32}{subsection.4.1.2}}
\newlabel{personDetection}{{\M@TitleReference {4.1.2}{Person Segmentation}}{32}{Person Segmentation}{section*.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces The overall data-flow of the data pre-processing module}}{33}{figure.4.2}}
\newlabel{fig:vpp}{{\M@TitleReference {4.2}{The overall data-flow of the data pre-processing module}}{33}{The overall data-flow of the data pre-processing module}{figure.4.2}{}}
\citation{inria_person}
\newlabel{pre_processing}{{\M@TitleReference {4.1.2}{Data pre-processing}}{37}{Data pre-processing}{section*.15}{}}
\newlabel{augmentation}{{\M@TitleReference {1}{Data pre-processing}}{37}{Data pre-processing}{Item.35}{}}
\newlabel{down-sampling}{{\M@TitleReference {2}{Data pre-processing}}{37}{Data pre-processing}{Item.36}{}}
\newlabel{normalization}{{\M@TitleReference {4}{Data pre-processing}}{37}{Data pre-processing}{Item.38}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Horizontal flipping of a video}}{38}{figure.4.3}}
\newlabel{fig:flip_4}{{\M@TitleReference {4.3}{Horizontal flipping of a video}}{38}{Horizontal flipping of a video}{figure.4.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Random cropping a video}}{38}{figure.4.4}}
\newlabel{fig:cropping_4}{{\M@TitleReference {4.4}{Random cropping a video}}{38}{Random cropping a video}{figure.4.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.3}Feature descriptor}{38}{subsection.4.1.3}}
\newlabel{3dconv_layers}{{\M@TitleReference {4.1.3}{Feature descriptor}}{38}{Feature descriptor}{subsection.4.1.3}{}}
\citation{bn}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces The structure of the feature descriptor}}{39}{figure.4.5}}
\newlabel{fig:feature_descriptor}{{\M@TitleReference {4.5}{The structure of the feature descriptor}}{39}{The structure of the feature descriptor}{figure.4.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces The architecture of the 3D ConvNet}}{39}{figure.4.6}}
\newlabel{fig:3DConvNet_4}{{\M@TitleReference {4.6}{The architecture of the 3D ConvNet}}{39}{The architecture of the 3D ConvNet}{figure.4.6}{}}
\newlabel{3dconv_filters}{{\M@TitleReference {4.1.3}{3D convolution:}}{39}{3D convolution:}{section*.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces The illustration of the ReLU operation}}{40}{figure.4.7}}
\newlabel{fig:relu}{{\M@TitleReference {4.7}{The illustration of the ReLU operation}}{40}{The illustration of the ReLU operation}{figure.4.7}{}}
\newlabel{bn}{{\M@TitleReference {4.1.3}{Batch Normalization}}{40}{Batch Normalization}{section*.19}{}}
\citation{Tran2015}
\citation{3dcnn_1}
\citation{dropout}
\newlabel{pooling}{{\M@TitleReference {4.1.3}{Max pooling:}}{41}{Max pooling:}{section*.20}{}}
\newlabel{fc}{{\M@TitleReference {4.1.3}{Fully connected layer}}{41}{Fully connected layer}{section*.21}{}}
\newlabel{dropout}{{\M@TitleReference {4.1.3}{Fully connected layer}}{41}{Fully connected layer}{section*.21}{}}
\citation{adam}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.4}Softmax classifier}{42}{subsection.4.1.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.5}Optimizer}{42}{subsection.4.1.5}}
\newlabel{Initialization}{{\M@TitleReference {4.1.5}{Initialization of the parameters}}{42}{Initialization of the parameters}{section*.23}{}}
\citation{bpa}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces The softmax classifier}}{43}{figure.4.8}}
\newlabel{fig:softmax}{{\M@TitleReference {4.8}{The softmax classifier}}{43}{The softmax classifier}{figure.4.8}{}}
\newlabel{optimizer}{{\M@TitleReference {4.1.5}{Optimizing of the parameters}}{43}{Optimizing of the parameters}{section*.24}{}}
\newlabel{learning_rate}{{\M@TitleReference {4.1.5}{Dynamic adjustment of the learning rate}}{44}{Dynamic adjustment of the learning rate}{section*.25}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Interaction detection}{44}{section.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Data-flow of interaction detection}{44}{subsection.4.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces The data-flow of interaction detection}}{45}{figure.4.9}}
\newlabel{fig:interaction_detection}{{\M@TitleReference {4.9}{The data-flow of interaction detection}}{45}{The data-flow of interaction detection}{figure.4.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Detection of all people}{45}{subsection.4.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.10}{\ignorespaces Illustrations of notations of bounding boxes, a). notations of a bounding boxes of an individual and b). notations of a bounding box of two-person interaction.}}{46}{figure.4.10}}
\newlabel{fig:bb_notations}{{\M@TitleReference {4.10}{Illustrations of notations of bounding boxes, a). notations of a bounding boxes of an individual and b). notations of a bounding box of two-person interaction.}}{46}{Illustrations of notations of bounding boxes, a). notations of a bounding boxes of an individual and b). notations of a bounding box of two-person interaction}{figure.4.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Spatial detection and tracking of interacting people}{46}{subsection.4.2.3}}
\newlabel{locate_interacting_people}{{\M@TitleReference {4.2.3}{Spatial detection and tracking of interacting people}}{46}{Spatial detection and tracking of interacting people}{subsection.4.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.11}{\ignorespaces Some examples of the horizon positions of the interacting people, sampled from the unsegmented videos of UT-Interaction dataset.}}{47}{figure.4.11}}
\newlabel{fig:interacting_people}{{\M@TitleReference {4.11}{Some examples of the horizon positions of the interacting people, sampled from the unsegmented videos of UT-Interaction dataset.}}{47}{Some examples of the horizon positions of the interacting people, sampled from the unsegmented videos of UT-Interaction dataset}{figure.4.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.12}{\ignorespaces An example illustration of preserving and discarding bounding boxes according to the value of \(Y_{Imean}\)}}{49}{figure.4.12}}
\newlabel{fig:y_mean}{{\M@TitleReference {4.12}{An example illustration of preserving and discarding bounding boxes according to the value of \(Y_{Imean}\)}}{49}{An example illustration of preserving and discarding bounding boxes according to the value of \(Y_{Imean}\)}{figure.4.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.4}Generate candidates of interaction video clips}{50}{subsection.4.2.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.13}{\ignorespaces An example illustration of preserving and discarding bounding boxes according to the value of \(X_{Imean}\)}}{51}{figure.4.13}}
\newlabel{fig:x_mean}{{\M@TitleReference {4.13}{An example illustration of preserving and discarding bounding boxes according to the value of \(X_{Imean}\)}}{51}{An example illustration of preserving and discarding bounding boxes according to the value of \(X_{Imean}\)}{figure.4.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.14}{\ignorespaces The illustration of temporally sliding the 3D windows which are used to generate candidates of interaction video clips.}}{52}{figure.4.14}}
\newlabel{fig:sliding_window}{{\M@TitleReference {4.14}{The illustration of temporally sliding the 3D windows which are used to generate candidates of interaction video clips.}}{52}{The illustration of temporally sliding the 3D windows which are used to generate candidates of interaction video clips}{figure.4.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.5}Classification of interaction video clips}{52}{subsection.4.2.5}}
\newlabel{extra_class}{{\M@TitleReference {4.2.5}{Training of interaction classifier for detection task}}{52}{Training of interaction classifier for detection task}{section*.34}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.6}Temporal combination}{52}{subsection.4.2.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.15}{\ignorespaces A diagram of temporally combining of interaction video clips }}{53}{figure.4.15}}
\newlabel{fig:temporal_combination}{{\M@TitleReference {4.15}{A diagram of temporally combining of interaction video clips }}{53}{A diagram of temporally combining of interaction video clips }{figure.4.15}{}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {5}Experimental Results}{55}{chapter.5}}
\newlabel{chap5}{{\M@TitleReference {5}{Experimental Results}}{55}{Experimental Results}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Searching of the optimal parameters}{55}{section.5.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}Model}{55}{subsection.5.1.1}}
\newlabel{tuning_network}{{\M@TitleReference {5.1.1}{Model}}{55}{Model}{subsection.5.1.1}{}}
\citation{xavier}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces The architecture of the singleNet}}{56}{figure.5.1}}
\newlabel{fig:arch_eval}{{\M@TitleReference {5.1}{The architecture of the singleNet}}{56}{The architecture of the singleNet}{figure.5.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}Assumptions and Factors and the default parameter setting}{56}{subsection.5.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.3}Experiments}{56}{subsection.5.1.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.3.1}Initialization of the network parameters}{56}{subsubsection.5.1.3.1}}
\@writefile{toc}{\contentsline {paragraph}{Experimental scheme}{56}{section*.35}}
\@writefile{toc}{\contentsline {paragraph}{Experimental results and analysis}{56}{section*.36}}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Default parameters for network and data pre-processing}}{57}{table.5.1}}
\newlabel{table:default_paras}{{\M@TitleReference {5.1}{Default parameters for network and data pre-processing}}{57}{Default parameters for network and data pre-processing}{table.5.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces The loss and classification accuracy vs. training epochs comparison between "Xavier" and normal distribution initialization.}}{58}{figure.5.2}}
\newlabel{fig:plot_xavier_vs_normal}{{\M@TitleReference {5.2}{The loss and classification accuracy vs. training epochs comparison between "Xavier" and normal distribution initialization.}}{58}{The loss and classification accuracy vs. training epochs comparison between "Xavier" and normal distribution initialization}{figure.5.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.3.2}Learning Rate}{58}{subsubsection.5.1.3.2}}
\@writefile{toc}{\contentsline {paragraph}{Experimental scheme}{58}{section*.37}}
\@writefile{toc}{\contentsline {paragraph}{Experimental results and analysis}{58}{section*.38}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces The loss and classification accuracy vs. training epochs comparison between different initial biases.}}{59}{figure.5.3}}
\newlabel{fig:plot_biases}{{\M@TitleReference {5.3}{The loss and classification accuracy vs. training epochs comparison between different initial biases.}}{59}{The loss and classification accuracy vs. training epochs comparison between different initial biases}{figure.5.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.3.3}Dropout layer}{59}{subsubsection.5.1.3.3}}
\@writefile{toc}{\contentsline {paragraph}{Experimental scheme}{59}{section*.39}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces The loss and classification accuracy vs. training epochs comparison between networks with different learning rate.}}{60}{figure.5.4}}
\newlabel{fig:plot_lr}{{\M@TitleReference {5.4}{The loss and classification accuracy vs. training epochs comparison between networks with different learning rate.}}{60}{The loss and classification accuracy vs. training epochs comparison between networks with different learning rate}{figure.5.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Experimental results and analysis}{60}{section*.40}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces The loss and classification accuracy vs. training epochs comparison between different keep probabilities in the dropout layers.}}{61}{figure.5.5}}
\newlabel{fig:plot_dropout}{{\M@TitleReference {5.5}{The loss and classification accuracy vs. training epochs comparison between different keep probabilities in the dropout layers.}}{61}{The loss and classification accuracy vs. training epochs comparison between different keep probabilities in the dropout layers}{figure.5.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.3.4}Batch normalization layer}{61}{subsubsection.5.1.3.4}}
\@writefile{toc}{\contentsline {paragraph}{Experimental scheme}{61}{section*.41}}
\@writefile{toc}{\contentsline {paragraph}{Experimental results and analysis}{61}{section*.42}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces The loss and classification accuracy vs. training epochs comparison between networks with and without batch normalization layers.}}{62}{figure.5.6}}
\newlabel{fig:plot_bn_en}{{\M@TitleReference {5.6}{The loss and classification accuracy vs. training epochs comparison between networks with and without batch normalization layers.}}{62}{The loss and classification accuracy vs. training epochs comparison between networks with and without batch normalization layers}{figure.5.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.3.5}The number of the convolutional layers}{62}{subsubsection.5.1.3.5}}
\@writefile{toc}{\contentsline {paragraph}{Experimental scheme}{62}{section*.43}}
\@writefile{toc}{\contentsline {paragraph}{Experimental results and analysis}{62}{section*.44}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces The structures and parameters of the three 3DConvNets which have different number of convolutional layers.}}{63}{figure.5.7}}
\newlabel{fig:cnn_layers}{{\M@TitleReference {5.7}{The structures and parameters of the three 3DConvNets which have different number of convolutional layers.}}{63}{The structures and parameters of the three 3DConvNets which have different number of convolutional layers}{figure.5.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.3.6}The size of the convolutional kernel}{63}{subsubsection.5.1.3.6}}
\@writefile{toc}{\contentsline {paragraph}{Experimental scheme}{63}{section*.45}}
\@writefile{toc}{\contentsline {paragraph}{Experimental results and analysis}{63}{section*.46}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.3.7}The number of filters of each convolutional layer}{63}{subsubsection.5.1.3.7}}
\@writefile{toc}{\contentsline {paragraph}{Experimental scheme}{63}{section*.47}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces The loss and classification accuracy vs. training epochs comparison between networks with different convolutional layers.}}{64}{figure.5.8}}
\newlabel{fig:plot_layers}{{\M@TitleReference {5.8}{The loss and classification accuracy vs. training epochs comparison between networks with different convolutional layers.}}{64}{The loss and classification accuracy vs. training epochs comparison between networks with different convolutional layers}{figure.5.8}{}}
\@writefile{toc}{\contentsline {paragraph}{Experimental results and analysis}{64}{section*.48}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.3.8}The number of neurons of each fully connected layer}{64}{subsubsection.5.1.3.8}}
\@writefile{toc}{\contentsline {paragraph}{Experimental scheme}{64}{section*.49}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces The loss and classification accuracy vs. training epochs comparison between networks with different sizes of the convolutional kernels.}}{65}{figure.5.9}}
\newlabel{fig:plot_cnn_kernel}{{\M@TitleReference {5.9}{The loss and classification accuracy vs. training epochs comparison between networks with different sizes of the convolutional kernels.}}{65}{The loss and classification accuracy vs. training epochs comparison between networks with different sizes of the convolutional kernels}{figure.5.9}{}}
\@writefile{toc}{\contentsline {paragraph}{Experimental results and analysis}{65}{section*.50}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.10}{\ignorespaces The loss and classification accuracy vs. training epochs comparison between networks with different number of filters in each convolutional layer.}}{66}{figure.5.10}}
\newlabel{fig:plot_nof}{{\M@TitleReference {5.10}{The loss and classification accuracy vs. training epochs comparison between networks with different number of filters in each convolutional layer.}}{66}{The loss and classification accuracy vs. training epochs comparison between networks with different number of filters in each convolutional layer}{figure.5.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.3.9}Data augmentation: horizontal flipping}{66}{subsubsection.5.1.3.9}}
\@writefile{toc}{\contentsline {paragraph}{Experimental scheme}{66}{section*.51}}
\@writefile{toc}{\contentsline {paragraph}{Experimental results and analysis}{66}{section*.52}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.11}{\ignorespaces The loss and classification accuracy vs. training epochs comparison between networks with different number of output neurons in each fully connected layer.}}{67}{figure.5.11}}
\newlabel{fig:plot_noo}{{\M@TitleReference {5.11}{The loss and classification accuracy vs. training epochs comparison between networks with different number of output neurons in each fully connected layer.}}{67}{The loss and classification accuracy vs. training epochs comparison between networks with different number of output neurons in each fully connected layer}{figure.5.11}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.3.10}Data augmentation: random cropping}{67}{subsubsection.5.1.3.10}}
\@writefile{toc}{\contentsline {paragraph}{Experimental scheme}{67}{section*.53}}
\@writefile{toc}{\contentsline {paragraph}{Experimental results and analysis}{67}{section*.54}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.12}{\ignorespaces The loss and classification accuracy vs. training epochs comparison between networks trained on the data with and without horizontal flipping.}}{68}{figure.5.12}}
\newlabel{fig:plot_rlf}{{\M@TitleReference {5.12}{The loss and classification accuracy vs. training epochs comparison between networks trained on the data with and without horizontal flipping.}}{68}{The loss and classification accuracy vs. training epochs comparison between networks trained on the data with and without horizontal flipping}{figure.5.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.3.11}Temporal down-sampling of frames}{68}{subsubsection.5.1.3.11}}
\@writefile{toc}{\contentsline {paragraph}{Experimental scheme}{68}{section*.55}}
\@writefile{toc}{\contentsline {paragraph}{Experimental results and analysis}{68}{section*.56}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.13}{\ignorespaces The loss and classification accuracy vs. training epochs comparison between networks trained on training data with different random cropping configurations.}}{69}{figure.5.13}}
\newlabel{fig:plot_rc}{{\M@TitleReference {5.13}{The loss and classification accuracy vs. training epochs comparison between networks trained on training data with different random cropping configurations.}}{69}{The loss and classification accuracy vs. training epochs comparison between networks trained on training data with different random cropping configurations}{figure.5.13}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.3.12}Data normalization}{69}{subsubsection.5.1.3.12}}
\@writefile{toc}{\contentsline {paragraph}{Experimental scheme}{69}{section*.57}}
\@writefile{toc}{\contentsline {paragraph}{Experimental results and analysis}{69}{section*.58}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.14}{\ignorespaces The loss and classification accuracy vs. training epochs comparison between networks trained on training data with different temporal down-sampling strides.}}{70}{figure.5.14}}
\newlabel{fig:plot_tds}{{\M@TitleReference {5.14}{The loss and classification accuracy vs. training epochs comparison between networks trained on training data with different temporal down-sampling strides.}}{70}{The loss and classification accuracy vs. training epochs comparison between networks trained on training data with different temporal down-sampling strides}{figure.5.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.4}Conclusions of parameter searching}{70}{subsection.5.1.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.15}{\ignorespaces The loss and classification accuracy vs. training epochs comparison between networks trained on training data with different normalization methods.}}{71}{figure.5.15}}
\newlabel{fig:plot_dnm}{{\M@TitleReference {5.15}{The loss and classification accuracy vs. training epochs comparison between networks trained on training data with different normalization methods.}}{71}{The loss and classification accuracy vs. training epochs comparison between networks trained on training data with different normalization methods}{figure.5.15}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}The experimental results of of the interaction classification}{71}{section.5.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Train the network from scratch}{71}{subsection.5.2.1}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}The experimental results of of the interaction detection}{72}{section.5.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}The results of the interacting people detection}{72}{subsection.5.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}The results of interaction detection}{72}{subsection.5.3.2}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Discussion}{72}{section.5.4}}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces Parameter, network architecture and training settings for network and data pre-processing.}}{73}{table.5.2}}
\newlabel{table:network_settings}{{\M@TitleReference {5.2}{Parameter, network architecture and training settings for network and data pre-processing.}}{73}{Parameter, network architecture and training settings for network and data pre-processing}{table.5.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.16}{\ignorespaces Sample frames of the results of interacting people detection}}{74}{figure.5.16}}
\newlabel{fig:int_det}{{\M@TitleReference {5.16}{Sample frames of the results of interacting people detection}}{74}{Sample frames of the results of interacting people detection}{figure.5.16}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.3}{\ignorespaces The experimental results of the three models' classification accuracy, evaluated on UT-Interaction set1}}{75}{table.5.3}}
\newlabel{table:threeModels}{{\M@TitleReference {5.3}{The experimental results of the three models' classification accuracy, evaluated on UT-Interaction set1}}{75}{The experimental results of the three models' classification accuracy, evaluated on UT-Interaction set1}{table.5.3}{}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {6}Conclusion}{77}{chapter.6}}
\newlabel{chap6}{{\M@TitleReference {6}{Conclusion}}{77}{Conclusion}{chapter.6}{}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {appendix}{\chapternumberline {A}Appendix A}{79}{appendix.A}}
\newlabel{app:app01}{{\M@TitleReference {A}{Appendix A}}{79}{Appendix A}{appendix.A}{}}
\bibstyle{siam}
\bibdata{thesisbiblio}
\bibcite{3dcnn_1}{{1}{}{{}}{{}}}
\bibcite{choi2012}{{2}{}{{}}{{}}}
\bibcite{hog}{{3}{}{{}}{{}}}
\bibcite{inria_person}{{4}{}{{}}{{}}}
\bibcite{hof}{{5}{}{{}}{{}}}
\bibcite{bov}{{6}{}{{}}{{}}}
\bibcite{grepory2010}{{7}{}{{}}{{}}}
\bibcite{xavier}{{8}{}{{}}{{}}}
\bibcite{kaiming}{{9}{}{{}}{{}}}
\bibcite{activitynet200}{{10}{}{{}}{{}}}
\bibcite{bn}{{11}{}{{}}{{}}}
\bibcite{Ji2013}{{12}{}{{}}{{}}}
\bibcite{karpathy2014}{{13}{}{{}}{{}}}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{81}{section*.62}}
\bibcite{adam}{{14}{}{{}}{{}}}
\bibcite{alex2008}{{15}{}{{}}{{}}}
\bibcite{yu}{{16}{}{{}}{{}}}
\bibcite{kriz2012}{{17}{}{{}}{{}}}
\bibcite{cnn}{{18}{}{{}}{{}}}
\bibcite{lowe1999}{{19}{}{{}}{{}}}
\bibcite{lowe2004}{{20}{}{{}}{{}}}
\bibcite{marszalek09}{{21}{}{{}}{{}}}
\bibcite{narayan2014}{{22}{}{{}}{{}}}
\bibcite{Ng2015}{{23}{}{{}}{{}}}
\bibcite{ning2005}{{24}{}{{}}{{}}}
\bibcite{aslan}{{25}{}{{}}{{}}}
\bibcite{patron2010}{{26}{}{{}}{{}}}
\bibcite{bpa}{{27}{}{{}}{{}}}
\bibcite{ut2010}{{28}{}{{}}{{}}}
\bibcite{kth}{{29}{}{{}}{{}}}
\bibcite{paul2007}{{30}{}{{}}{{}}}
\bibcite{simonyan2014}{{31}{}{{}}{{}}}
\bibcite{ucf101}{{32}{}{{}}{{}}}
\bibcite{dropout}{{33}{}{{}}{{}}}
\bibcite{Tran2015}{{34}{}{{}}{{}}}
\bibcite{Gemeren2015}{{35}{}{{}}{{}}}
\bibcite{shakefive2}{{36}{}{{}}{{}}}
\bibcite{wang2012}{{37}{}{{}}{{}}}
\bibcite{wang2013}{{38}{}{{}}{{}}}
\bibcite{m2i_tju}{{39}{}{{}}{{}}}
\bibcite{zeiler2014}{{40}{}{{}}{{}}}
\bibcite{yimeng}{{41}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\memsetcounter{lastsheet}{98}
\memsetcounter{lastpage}{84}
